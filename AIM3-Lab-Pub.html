<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Title -->
    <title>AI·M³-Publication</title>

    <!-- Required Meta Tags Always Come First -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Favicon -->
    <link rel="shortcut icon" href="./images/favicon.png">

    <!-- Google Fonts -->
    <!-- <link href="https://fonts.googleapis.com/css?family=Barlow:300,400,400i,500,700%7CAlegreya:400" rel="stylesheet"> -->

    <!-- CSS Global Compulsory -->
    <link rel="stylesheet" href="./css/vendor/bootstrap/bootstrap.min.css">

    <!-- CSS Implementing Plugins -->
    <link rel="stylesheet" href="./css/vendor/icon-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="./css/vendor/icon-line-pro/style.css">
    <link rel="stylesheet" href="./css/vendor/icon-hs/style.css">
    <link rel="stylesheet" href="./css/vendor/icon-material/material-icons.css">
    <link rel="stylesheet" href="./css/vendor/animate.css">
    <link rel="stylesheet" href="./css/vendor/hs-megamenu/src/hs.megamenu.css">
    <link rel="stylesheet" href="./css/vendor/hamburgers/hamburgers.min.css">
    <link rel="stylesheet" href="./css/vendor/chosen/chosen.css">

    <!-- CSS Unify Theme -->
    <link rel="stylesheet" href="./css/styles.multipage-education.css">

    <!-- CSS Customization -->
    <link rel="stylesheet" href="./css/custom.css">
  </head>

  <body>
    <main>

      <!-- Header -->
      <header id="js-header" class="u-header">
        <div class="u-header__section">

          <div class="container">
            <!-- Nav -->
            <nav class="js-mega-menu navbar navbar-expand-lg g-px-0 g-py-5 g-py-0--lg">
              <!-- Logo -->
              <a class="navbar-brand g-max-width-170 g-max-width-200--lg" href="AIM3-Lab.html">
                <img class="img-fluid " src="./images/logo.png" alt="Logo">
              </a>
              <!-- End Logo -->

              <!-- Responsive Toggle Button -->
              <button class="navbar-toggler navbar-toggler-right btn g-line-height-1 g-brd-none g-pa-0" type="button"
                      aria-label="Toggle navigation"
                      aria-expanded="false"
                      aria-controls="navBar"
                      data-toggle="collapse"
                      data-target="#navBar">
                <span class="hamburger hamburger--slider g-px-0">
                  <span class="hamburger-box">
                    <span class="hamburger-inner"></span>
                  </span>
                </span>
              </button>
              <!-- End Responsive Toggle Button -->

              <!-- Navigation -->
              <div id="navBar" class="collapse navbar-collapse">
                <ul class="navbar-nav align-items-lg-center g-py-30 g-py-0--lg ml-auto">
                  <li class="nav-item">
                    <a class="nav-link g-color-primary--hover g-font-size-15 g-font-size-17--xl g-px-15--lg g-py-10 g-py-30--lg" href="AIM3-Lab.html">
                      Home
                    </a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link g-color-primary--hover g-font-size-15 g-font-size-17--xl g-px-15--lg g-py-10 g-py-30--lg" href="AIM3-Lab-Research.html">
                      Research
                    </a>
                  </li>
                  <li class="nav-item u-nav-v5">
                    <a  style="color:#a61d37" class="nav-link g-color-primary--hover g-font-size-15 g-font-size-17--xl g-px-15--lg g-py-10 g-py-30--lg" href="AIM3-Lab-Pub.html">
                      Publications
                    </a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link g-color-primary--hover g-font-size-15 g-font-size-17--xl g-px-15--lg g-py-10 g-py-30--lg" href="AIM3-Lab-People.html">
                      People
                    </a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link g-color-primary--hover g-font-size-15 g-font-size-17--xl g-pl-15--lg g-pr-0--lg g-py-10 g-py-30--lg" href="AIM3-Lab-Awards.html">
                      Awards
                    </a>
                  </li>
                </ul>
              </div>
              <!-- End Navigation -->
            </nav>
            <!-- End Nav -->
          </div>
        </div>
      </header>
      <!-- End Header -->

      <!-- Publications -->
      <div class="g-bg-secondary">
        <!-- Programs -->
        <div class="container g-pt-70 ">
          <div class="row">
            <div class="col-md-12 col-lg-12 g-mb-70">
              <div class="mb-5">
                <h2>Journal and Conference</h2>
              </div>

              <!--div class="g-px-15 mb-5">
                <Heading>
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >
                        Paper
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >
                        Publisher
                      </a></h3>
                    </div>
                  </div>
                </div-->
                <!-- End Heading -->


              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2022
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <!-- paper list -->

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Multi-Lingual Acquisition on Multimodal Pre-training for Cross-modal Retrieval
                <br>Liang Zhang, Anwen Hu, Qin Jin<br><a style="color:#a61d37" href="https://aclanthology.org/2022.coling-1.57.pdf" target="https://aclanthology.org/2022.coling-1.57.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   NeurIPS, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>DialogueEIN: Emotion Interaction Network for Dialogue Affective Analysis</b>
                <br>Yuchen Liu, Jinming Zhao, Jingwen Hu, Ruichen Li, Qin Jin<br><a style="color:#a61d37" href="https://aclanthology.org/2022.coling-1.57.pdf" target="https://aclanthology.org/2022.coling-1.57.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   COLING, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval</b>
                <br>Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2207.07852.pdf" target="https://arxiv.org/pdf/2207.07852.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ECCV, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning</b>
                <br>Sipeng Zheng, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="" target="">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ECCV, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Unifying Event Detection and Captioning as Sequence Generation via Pre-Training</b>
                <br>Qi Zhang, Yuqing Song, Qin Jin<br><a style="color:#a61d37" href="http://dx.doi.org/10.48550/arXiv.2207.08625" target="http://dx.doi.org/10.48550/arXiv.2207.08625">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ECCV, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Progressive Learning for Image Retrieval with Hybrid-Modality Queries</b>
                <br>Yida Zhao, Yuqing Song, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2204.11212.pdf" target="https://arxiv.org/pdf/2204.11212.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   SIGIR, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>VRDFormer: End-to-End Video Visual Relation Detection with Transformers</b>
                <br>Sipeng Zheng, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf" target="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf"></a></span>
                </div>
                <div class="col-sm-2">
                   CVPR, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database</b>
                <br>Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin Jin, Xinchao Wang, Haizhou Li<br><a style="color:#a61d37" href="https://github.com/AIM3-RUC/RUCM3ED" target="https://github.com/AIM3-RUC/RUCM3ED">[code]</a></span>
                </div>
                <div class="col-sm-2">
                   ACL, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Image Difference Captioning with Pre-Training and Contrastive Learning</b>
                <br>Linli Yao, Weiying Wang, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2202.04298.pdf" target="https://arxiv.org/pdf/2202.04298.pdf">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/yaolinli/IDC" target="https://github.com/yaolinli/IDC">[code]</a></span>
                </div>
                <div class="col-sm-2">
                   AAAI, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Training strategies for automatic song writing: a unified framework perspective</b>
                <br>Tao Qian, Jiatong Shi, Shuai Guo, Peter Wu, Qin Jin<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746818" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746818">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ICASSP, 2022.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>MEmoBERT: Pre-training Model with Prompt-based Learning for Multimodal Emotion Recognition</b>
                <br>Jinming Zhao, Ruichen Li, Qin Jin, Xinchao Wang, Haizhou Li<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2111.00865.pdf" target="https://arxiv.org/pdf/2111.00865.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ICASSP, 2022.
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2021
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training</b>
                <br>Yuqing Song, Shizhe Chen, Qin Jin, Wei Luo, Jun Xie, and Fei Huang</span><br><a style="color:#a61d37" href="https://arxiv.org/pdf/2108.11119.pdf" target="https://arxiv.org/pdf/2108.11119.pdf">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/syuqings/Fashion-MMT" target="https://github.com/syuqings/Fashion-MMT">[code]</a>
                </div>
                <div class="col-sm-2">
                   ACM Multimedia, 2021.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Question-controlled Text-aware Image Captioning</b>
                <br>Anwen Hu, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475452" target="https://dl.acm.org/doi/pdf/10.1145/3474085.3475452">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/HAWLYQ/Qc-TextCap" target="https://github.com/HAWLYQ/Qc-TextCap">[code]</a></span>
                </div>
                <div class="col-sm-2">
                   ACM Multimedia, 2021.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Enhancing Neural Machine Translation with Dual-Side Multimodal Awareness</b>
                <br>Yuqing Song, Shizhe Chen, Qin Jin, Wei Luo, Jun Xie, and Fei Huang<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/document/9465713" target="https://ieeexplore.ieee.org/document/9465713">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   IEEE Transactions on Multimedia, 2021.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Speech Emotion Recognition via Multi-Level Cross-Modal Distillation</b>
                <br>Ruichen Li, Jinming Zhao, Qin Jin<br><a style="color:#a61d37" href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21p_interspeech.pdf" target="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21p_interspeech.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   Interspeech, 2021.
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities</b>
                <br>Jinming Zhao, Ruichen Li, Qin Jin<br><a style="color:#a61d37" href="https://aclanthology.org/2021.acl-long.203.pdf" target="https://aclanthology.org/2021.acl-long.203.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                   ACL, 2021.
                </div>
              </div>

				      <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation</b>
                <br>Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2107.06779.pdf" target="https://arxiv.org/pdf/2107.06779.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                 ACL, 2021.  
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Towards Diverse Paragraph Captioning for Untrimmed Videos</b>
                <br>Yuqing Song, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2105.14477.pdf" target="https://arxiv.org/pdf/2105.14477.pdf">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/syuqings/video-paragraph" target="https://github.com/syuqings/video-paragraph">[code]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR, 2021. 
                </div>
              </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Sequence-To-Sequence Singing Voice Synthesis With Perceptual Entropy Loss</b><br>
                Jiatong Shi, Shuai Guo, Nan Huo, Yuekai Zhang, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2010.12024.pdf" target="https://arxiv.org/pdf/2010.12024.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICASSP, 2021. 
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2020
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>

              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Context-aware Goodness of Pronunciation for Computer-Assisted Pronunciation Trainings</b><br>
                Jiatong Shi, Nan Huo, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2008.08647.pdf" target="https://arxiv.org/pdf/2008.08647.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Interspeech, 2020. 
                </div>
              </div>				
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generations</b><br>
                Weiying Wang, Jieting Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413890" target="https://dl.acm.org/doi/pdf/10.1145/3394171.3413890">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/AIM3-RUC/VideoIC" target="https://github.com/AIM3-RUC/VideoIC">[code]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2020. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>ICECAP: Information Concentrated Entity-aware Image Captioning</b><br>
                Anwen Hu, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413576" target="https://dl.acm.org/doi/pdf/10.1145/3394171.3413576">[pdf]</a>&nbsp&nbsp&nbsp<a style="color:#a61d37" href="https://github.com/HAWLYQ/ICECAP" target="https://github.com/HAWLYQ/ICECAP">[code]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2020. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching</b><br>
                Jingjun Liang, Ruichen Li, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2009.02598.pdf" target="https://arxiv.org/pdf/2009.02598.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2020. 
                </div>
              </div>				
                
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs</b><br>
                Shizhe Chen, Qin Jin, Peng Wang, Qi Wu<br><a style="color:#a61d37" href="https://arxiv.org/pdf/2003.00387.pdf" target="https://arxiv.org/pdf/2003.00387.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR, 2020. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning</b><br>
                Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu<br><a style="color:#a61d37" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.pdf" target="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR, 2020. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Better Captioning With Sequence-Level Exploration</b><br>
                Jia Chen, Qin Jin<br><a style="color:#a61d37" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Better_Captioning_With_Sequence-Level_Exploration_CVPR_2020_paper.pdf" target="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Better_Captioning_With_Sequence-Level_Exploration_CVPR_2020_paper.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR, 2020. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Skeleton-based Interactive Graph Network for Human Object Interaction Detection</b><br>
                Sipeng Zheng, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102755" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102755">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICME, 2020. 
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2019
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data</b><br>
                Shizhe Chen, Qin Jin, Alexandar Hauptmann<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1906.00378.pdf" target="https://arxiv.org/pdf/1906.00378.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  AAAI, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Cross-culture Multimodal Emotion Recognition with Adversarial Learning</b><br>
                Jingjun Liang, Shizhe Chen, Jinming Zhao, Qin Jin, Haibo Liu, Li Lu<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683725" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683725">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICASSP, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Activitynet 2019 Task 3:Exploring Contexts for Dense Captioning Events in Video</b><br>
                Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin,Zhaoyang Zeng, Bei Liu, Jianlong Fu, Alexander Hauptmann<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1907.05092.pdf" target="https://arxiv.org/pdf/1907.05092.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR 2019, ActivityNet Large Scale Activity Recognition Challenge. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots</b><br>
                Shizhe Chen, Qin Jin, Jianlong Fu<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1906.00872.pdf" target="https://arxiv.org/pdf/1906.00872.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  IJCAI, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Generating Video Descriptions With Latent Topic Guidance</b><br>
                Shizhe Chen, Qin Jin, Jia Chen, Alexander G. Hauptmann<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1708.09666.pdf" target="https://arxiv.org/pdf/1708.09666.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  IEEE TRANSACTIONS ON MULTIMEDIA, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Speech Emotion Recognition in Dyadic Dialogues</b><br>
                Jinming Zhao, Shizhe Chen, Jingjun Liang, Qin Jin<br><a style="color:#a61d37" href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2103.pdf" target="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2103.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  INTERSPEECH, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards</b><br>
                Yuqing Song, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1908.05407.pdf" target="https://arxiv.org/pdf/1908.05407.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Visual Relation Detection with Multi-Level Attention</b><br>
                Sipeng Zheng, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350962" target="https://dl.acm.org/doi/pdf/10.1145/3343031.3350962">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Neural Storyboard Artist: Visualizing Stories with Coherent Image Sequences</b><br>
                Shizhe Chen, Bei Liu, Jianlong Fu, Ruihua Song, Qin Jin, Pingping Lin, Xiaoyu Qi, Chunting Wang, Jin Zhou<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1911.10460.pdf" target="https://arxiv.org/pdf/1911.10460.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Relation Understanding in Videos</b><br>
                Sipeng Zheng, Xiangyu Chen, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3343031.3356080" target="https://dl.acm.org/doi/pdf/10.1145/3343031.3356080">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, Grand Challenge: Relation Understanding in Videos, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Adversarial Domain Adaption for Multi-Cultural DimensionalEmotion Recognition in Dyadic Interactions</b><br>
                Jinming Zhao, Ruichen Li, Jingjun Liang, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3347320.3357692" target="https://dl.acm.org/doi/pdf/10.1145/3347320.3357692">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  AVEC, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Integrating Temporal and Spatial Attentions for VATEX Video Captioning Challenge 2019</b><br>
                Shizhe Chen, Yida Zhao, Yuqing Song, Qin Jin, Qi Wu<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1910.06737.pdf" target="https://arxiv.org/pdf/1910.06737.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICCV, VATEX Video Captioning Challenge 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension</b><br>
                Weiying Wang, Yongcheng Wang, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://aclanthology.org/D19-1517.pdf" target="https://aclanthology.org/D19-1517.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  EMNLP, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>RUC_AIM3 at TRECVID 2019: Video to Text</b><br>
                Yuqing Song, Yida Zhao, Shizhe Chen, Qin Jinn<br><a style="color:#a61d37" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/ruc_aim3.pdf" target="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/ruc_aim3.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  NIST TRECVID, 2019. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Semi-supervised Multimodal Emotion Recognition With Improved Wasserstein GANs</b><br>
                Jingjun Liang, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9023144" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9023144">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  APSIPA ASC, 2019. 
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2018
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>RUC+CMU: System Report for Dense Captioning Events in Videos</b><br>
                Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin, Alexandar Hauptmann<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1806.08854.pdf" target="https://arxiv.org/pdf/1806.08854.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  CVPR ActivityNet Large Scale Activity Recognition Challenge, 2018. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Class-aware Self-Attention for Audio Event Recognition</b><br>
                Shizhe Chen, Jia Chen, Qin Jin, Alexandar Hauptmann<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3206025.3206067" target="https://dl.acm.org/doi/pdf/10.1145/3206025.3206067">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMR, 2018. (Best Paper Runner-up) </span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Multimodal Dimensional and Continuous Emotion Recognition in Dyadic Video Interactions</b><br>
                Jinming Zhao, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://www.researchgate.net/publication/327723823_Multimodal_Dimensional_and_Continuous_Emotion_Recognition_in_Dyadic_Video_Interactions_19th_Pacific-Rim_Conference_on_Multimedia_Hefei_China_September_21-22_2018_Proceedings_Part_I" target="https://www.researchgate.net/publication/327723823_Multimodal_Dimensional_and_Continuous_Emotion_Recognition_in_Dyadic_Video_Interactions_19th_Pacific-Rim_Conference_on_Multimedia_Hefei_China_September_21-22_2018_Proceedings_Part_I">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Pacific-Rim Conference on Multimedia (PCM), 2018. </span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>iMakeup: Makeup Instructional Video Dataset for Fine-grained Dense Video Captioning</b><br>
                Xiaozhu Lin, Qin Jin, Shizhe Chen, Yuqing Song, Yida Zhao<br><a style="color:#a61d37" href="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_X_Lin.pdf" target="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_X_Lin.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Pacific-Rim Conference on Multimedia (PCM), 2018. </span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Multi-modal Multi-cultural Dimensional Continues Emotion Recognition in Dyadic Interactions</b><br>
                Jinming Zhao, Ruichen Li, Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3266302.3266313" target="https://dl.acm.org/doi/pdf/10.1145/3266302.3266313">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia Audio-Visual Emotion Challenge (AVEC) Workshop, 2018. </span><br>
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2017
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Video Captioning with Guidance of Multimodal Latent Topics</b><br>
                Shizhe Chen, Jia Chen, Qin Jin, Alexandar Hauptmann<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1708.09667.pdf" target="https://arxiv.org/pdf/1708.09667.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2017. </span><br>                                 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Knowing Yourself: Improving Video Caption via In-depth Recap</b><br>
                Qin Jin, Shizhe Chen, Jia Chen, Alexandar Hauptmann<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3123266.3127901" target="https://dl.acm.org/doi/pdf/10.1145/3123266.3127901">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2017. (Best Grand Challenge Paper) </span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Multimodal Multi-task Learning for Dimensional and Continuous Emotion Recognition</b><br>
                Shizhe Chen, Qin Jin, Jinming Zhao and Shuai Wang<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3133944.3133949" target="https://dl.acm.org/doi/pdf/10.1145/3133944.3133949">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia Audio-Visual Emotion Challenge (AVEC) Workshop, 2017. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Generating Video Descriptions with Topic Guidance</b><br>
                Shizhe Chen, Jia Chen, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1708.09666.pdf" target="https://arxiv.org/pdf/1708.09666.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMR, 2017.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Emotion Recognition with Multimodal Features and Temporal Models</b><br>
                Shuai Wang, Wenxuan Wang, Jinming Zhao, Shizhe Chen, Qin Jin, Shilei Zhang, Yong Qin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/3136755.3143016" target="https://dl.acm.org/doi/pdf/10.1145/3136755.3143016">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMI, 2017.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Facial Action Units Detection with Multi-Features and-AUs Fusion</b><br>
                Xinrui Li, Shizhe Chen, and Qin Jin<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7961833" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7961833">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Automatic Face &amp; Gesture Recognition (FGR), 2017.
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2016
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Boosting Recommendation in Unexplored Categories by User Price Preference</b><br>
                Jia Chen, Qin Jin, Shiwan Zhao, Shenghua Bao, Li Zhang, Zhong Su, Yong Yu<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2978579" target="https://dl.acm.org/doi/pdf/10.1145/2978579">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Transactions on Information Systems (TOIS), 2016.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Video Emotion Recognition in the Wild Based on Fusion of Multimodal Features</b><br>
                Shizhe Chen, Xinrui Li, Qin Jin, Shilei Zhang, Yong Qin<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2993148.2997629" target="https://dl.acm.org/doi/pdf/10.1145/2993148.2997629">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMI 2016.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Describing Videos using Multi-modal Fusion</b><br>
                Qin Jin, Jia Chen, Shizhe Chen, Yifan Xiong<br><a style="color:#a61d37" href="https://www.jin-qin.com/papers/Describing_Videos_using_Multi-modal_Fusion.pdf" target="https://www.jin-qin.com/papers/Describing_Videos_using_Multi-modal_Fusion.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia, 2016. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Semantic Image Profiling for Historic Events: Linking Images to Phrases</b><br>
                Jia Chen, Qin Jin, Yifan Xiong<br><a style="color:#a61d37" href="https://people.cs.clemson.edu/~jzwang/ustc17/mm2016/p1028-chen.pdf" target="https://people.cs.clemson.edu/~jzwang/ustc17/mm2016/p1028-chen.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia 2016. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction</b><br>
                Shizhe Chen, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1709.02251.pdf" target="https://arxiv.org/pdf/1709.02251.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia 2016.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>History Rhyme: Searching Historic Events by Multimedia Knowledge</b><br>
                Yifan Xiong, Jia Chen, Qin Jin, Chao Zhang<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2964284.2973832" target="https://dl.acm.org/doi/pdf/10.1145/2964284.2973832">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia 2016. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Detecting Violence in Video using Subclasses</b><br>
                Xirong Li, Yujia Huo, Qin Jin, Jieping Xu<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2964284.2967289" target="https://dl.acm.org/doi/pdf/10.1145/2964284.2967289">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia 2016. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Generating Natural Video Descriptions via Multimodal Processing</b><br>
                Qin Jin, Junwei Liang, Xiaozhu Lin<br><a style="color:#a61d37" href="https://www.jin-qin.com/papers/Generating_Natural_Video_Descriptions_via_Multimodal_Processing.pdf" target="https://www.jin-qin.com/papers/Generating_Natural_Video_Descriptions_via_Multimodal_Processing.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Interspeech 2016. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Improving Image Captioning by Concept-based Sentence Reranking</b><br>
                Xirong Li, Qin Jin<br><a style="color:#a61d37" href="https://arxiv.org/pdf/1605.00855.pdf" target="https://arxiv.org/pdf/1605.00855.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Pacific-Rim Conference on Multimedia (PCM), 2016. (Best Paper Runner-up)
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Video Description Generation using Audio and Visual Cues</b><br>
                Qin Jin, Junwei Liang<br><a style="color:#a61d37" href="https://www.jin-qin.com/papers/Video_Description_Generation_using_Audio_and_Visual_Cues.pdf" target="https://www.jin-qin.com/papers/Video_Description_Generation_using_Audio_and_Visual_Cues.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMR 2016.
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2015
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Exploitation and Exploration Balanced Hierarchical Summary for Landmark Images</b><br>
                Jia Chen, Qin Jin, Shenghua Bao, Junfeng Ye, Zhong Su, Shimin Chen, Yong Yu<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7165653" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7165653">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  IEEE Transactions on Multimedia (TMM), 2015
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Lead Curve Detection in Drawings with Complex Cross-Points</b><br>
                Jia Chen, Min Li, Qin Jin, Yongzhe Zhang, Shenghua Bao, Zhong Su, Yong Yu<br><a style="color:#a61d37" href="https://www.jin-qin.com/papers/Lead_Curve_Detection_in_Drawings_with_Complex_Cross-Points.pdf" target="https://www.jin-qin.com/papers/Lead_Curve_Detection_in_Drawings_with_Complex_Cross-Points.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Neurocomputing, 2015, 168: 35-46.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Image Profiling for History Events on the Fly</b><br>
                Jia Chen, Qin Jin, Yong Yu, Alexander G. Hauptmann<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2964284.2964306" target="https://dl.acm.org/doi/pdf/10.1145/2964284.2964306">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ACM Multimedia 2015.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Persistent B+-Trees in Non-Volatile Main Memory</b><br>
                Shimin Chen and Qin Jin<br><a style="color:#a61d37" href="http://www.vldb.org/pvldb/vol8/p786-chen.pdf" target="http://www.vldb.org/pvldb/vol8/p786-chen.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  VLDB, Hawaii, USA, 2015 (VLDB’15).
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Semantic Concept Annotation for User Generated Videos Using Soundtracks</b><br>
                Qin Jin, Junwei Liang, Xixi He, Gang Yang, Jieping Xu, Xirong Li<br><a style="color:#a61d37" href="https://www.jin-qin.com/papers/Semantic_Concept_Annotation_for_User_Generated_Videos_Using_Soundtracks.pdf" target="https://www.jin-qin.com/papers/Semantic_Concept_Annotation_for_User_Generated_Videos_Using_Soundtracks.pdf">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICMR 2015.
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Speech Emotion Recognition With Acoustic And Lexical Features</b><br>
                Qin Jin, Chengxin Li, Shizhe Chen, Huimin Wu<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178872" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178872">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICASSP, 2015. 
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Detecting Semantic Concepts In Consumer Videos Using Audio</b><br>
                Junwei Liang, Qin Jin, Xixi He, Gang Yang, Jieping Xu, Xirong Li<br><a style="color:#a61d37" href="https://www.semanticscholar.org/paper/Detecting-semantic-concepts-in-consumer-videos-Liang-Jin/fb6adbc783d7b6bfb644042a27a2ed74145bbfac" target="https://www.semanticscholar.org/paper/Detecting-semantic-concepts-in-consumer-videos-Liang-Jin/fb6adbc783d7b6bfb644042a27a2ed74145bbfac">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ICASSP, 2015. 
                </div>
              </div>
              </div>

              <div class="g-px-15 mb-5">
                <!--Heading-->
                <div class="row g-bg-main g-color-white g-font-size-16 g-py-15">
                  <div class="col-sm-10">
                    <div class="d-flex align-items-center">
                      <h2 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover g-font-size-20" >
                        2014
                      </a></h2>
                    </div>
                  </div>
                  <div class="col-sm-2">
                    <div class="d-flex align-items-center">
                      <h3 class="h5 mb-0"><a class="u-link-v5 g-color-white-opacity-0_8 g-color-white--hover" >  
                      </a></h3>
                    </div>
                  </div>
                </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Does Product Recommendation Meet its Waterloo in Unexplored Categories? No, Price Comes to Help</b><br>
                Jia Chen, Qin Jin, Shiwan Zhao, Shenghua Bao, Li Zhang, Zhong Su, Yong Yu<br><a style="color:#a61d37" href="https://dl.acm.org/doi/pdf/10.1145/2600428.2609608" target="https://dl.acm.org/doi/pdf/10.1145/2600428.2609608">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  SIGIR 2014 (SIGIR’14).</span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b>Semantic Concept Annotation of Consumer Videos at Frame-level Using Audio</b><br>
                Junwei Liang, Qin Jin, Xixi He, Xirong Li, Gang Yang, Jieping Xu<br><a style="color:#a61d37" href="https://www.researchgate.net/publication/301952331_Semantic_Concept_Annotation_of_Consumer_Videos_at_Frame-Level_Using_Audio" target="https://www.researchgate.net/publication/301952331_Semantic_Concept_Annotation_of_Consumer_Videos_at_Frame-Level_Using_Audio">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  Pacific-rim Conference on Multimedia 2014 (PCM’14).</span><br>
                </div>
              </div>
              <div class="row g-brd-around g-brd-top-none g-brd-secondary-light-v2 g-font-size-16 g-py-15">
                <div class="col-sm-10">
                <span><b> Speech Emotion Classification using Acoustic Features</b><br>
                Shizhe Chen, Qin Jin, Xirong Li, Gang Yang, Jieping Xu<br><a style="color:#a61d37" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6936664" target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6936664">[pdf]</a></span>
                </div>
                <div class="col-sm-2">
                  ISCSLP, 2014.</span><br>
                </div>
              </div>
              </div>

            </div>

          </div>
        </div>
        <!-- End Programs -->
      </div>
      <!-- End Publications-->

      <!-- Footer -->
      <footer class="g-bg-secondary g-pt-100 g-pb-50">
        <div class="container">
          <!-- Footer Copyright -->
          <div class="row justify-content-lg-center align-items-center text-center">
            <div class="col-sm-5 col-md-5 col-lg-5 order-md-3 g-mb-30">
              <a class="u-link-v5 g-color-text g-color-primary--hover" >
                <i class="align-middle mr-2 icon-real-estate-027 u-line-icon-pro"></i>
                Renmin University, Beijing, China
              </a>
            </div>

            <div class="col-md-4 col-lg-3 order-md-1 g-mb-30">
              <p class="g-color-text mb-0">AI·M³ Lab</p>
            </div>
          </div>
          <!-- End Footer Copyright -->
        </div>
      </footer>
      <!-- End Footer -->

      <!-- Go to Top -->
      <a class="js-go-to u-go-to-v1 u-shadow-v32 g-width-40 g-height-40 g-color-primary g-color-white--hover g-bg-white g-bg-main--hover g-bg-main--focus g-font-size-12 rounded-circle" href="#" data-type="fixed" data-position='{
       "bottom": 15,
       "right": 15
     }' data-offset-top="400"
        data-compensation="#js-header"
        data-show-effect="slideInUp"
        data-hide-effect="slideInDown">
        <i class="hs-icon hs-icon-arrow-top"></i>
      </a>
      <!-- End Go to Top -->
    </main>

    <!-- JS Global Compulsory -->
    <script src="./css/vendor/jquery/jquery.min.js"></script>
    <script src="./css/vendor/jquery-migrate/jquery-migrate.min.js"></script>
    <script src="./css/vendor/popper.min.js"></script>
    <script src="./css/vendor/bootstrap/bootstrap.min.js"></script>

    <!-- JS Implementing Plugins -->
    <script src="./css/vendor/hs-megamenu/src/hs.megamenu.js"></script>
    <script src="./css/vendor/chosen/chosen.jquery.js"></script>

    <!-- JS Unify -->
    <script src="./js/hs.core.js"></script>
    <script src="./js/components/hs.header.js"></script>
    <script src="./js/helpers/hs.hamburgers.js"></script>
    <script src="./js/components/hs.dropdown.js"></script>
    <script src="./js/components/hs.select.js"></script>
    <script src="./js/components/hs.sticky-block.js"></script>
    <script src="./js/components/hs.go-to.js"></script>

    <!-- JS Customization -->
    <script src="./js/custom.js"></script>

    <!-- JS Plugins Init. -->
    <script>
      $(document).on('ready', function () {
        // initialization of header
        $.HSCore.components.HSHeader.init($('#js-header'));
        $.HSCore.helpers.HSHamburgers.init('.hamburger');

        // initialization of HSMegaMenu component
        $('.js-mega-menu').HSMegaMenu({
          event: 'hover',
          pageContainer: $('.container'),
          breakpoint: 991
        });

        // initialization of HSDropdown component
        $.HSCore.components.HSDropdown.init($('[data-dropdown-target]'), {
          afterOpen: function () {
            $(this).find('input[type="search"]').focus();
          }
        });

        // initialization of custom select
        $.HSCore.components.HSSelect.init('.js-custom-select');

        // initialization of sticky blocks
        setTimeout(function() {
          $.HSCore.components.HSStickyBlock.init('.js-sticky-block');
        }, 300);

        // initialization of go to
        $.HSCore.components.HSGoTo.init('.js-go-to');
      });
    </script>
  </body>
</html>
